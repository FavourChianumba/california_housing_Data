{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the scripts folder to the system path\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_data\n",
    "from preprocessing import preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models and Tools\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from housing.csv.\n"
     ]
    }
   ],
   "source": [
    "data = load_data('housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop NA values so that all rows have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stratifiedsplit import StratifiedSplit\n",
    "strat_train_set, strat_test_set = StratifiedSplit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified sampling ensures that the distribution of key categories in the training and testing sets matches that of the original dataset. This is especially important for a medium-sized dataset like this, where certain categories may be sparse.\n",
    "\n",
    "We remove the temporary income_cat column once the split is done, as it was only used for stratification.\n",
    "\n",
    "Afterwards we prepare training and testing data (x_train, y_train, x_test, y_test) from the stratified splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define x and y from the clean training and testing sets\n",
    "x_train = strat_train_set.drop(['median_house_value'], axis=1)\n",
    "y_train = strat_train_set['median_house_value']\n",
    "x_test = strat_test_set.drop(['median_house_value'], axis=1)\n",
    "y_test = strat_test_set['median_house_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joinng x and y training data so we can analyse some basic corrolations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x_train.join(y_train)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the 'ocean_proximity' column with `drop_first` to avoid multicollinearity issues\n",
    "train_data = pd.get_dummies(train_data, columns=['ocean_proximity'], drop_first=True)\n",
    "x_test = pd.get_dummies(x_test, columns=['ocean_proximity'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_points = [\n",
    "    # Southern California\n",
    "    (32.5343, -117.1232),  # Imperial Beach near San Diego (close to the border)\n",
    "    (32.8525, -117.2728),  # La Jolla Shores, San Diego\n",
    "    (33.5410, -117.7854),  # Laguna Beach\n",
    "    (34.0092, -118.4976),  # Santa Monica Pier\n",
    "    (34.0259, -118.7798),  # Malibu Lagoon State Beach\n",
    "\n",
    "    # Central Coast\n",
    "    (34.4140, -119.6846),  # East Beach, Santa Barbara\n",
    "    (35.2819, -120.6596),  # Avila Beach, San Luis Obispo\n",
    "    (36.5552, -121.9233),  # Point Lobos, Carmel Highlands\n",
    "    (36.9514, -122.0263),  # Santa Cruz Beach\n",
    "\n",
    "    # Bay Area\n",
    "    (37.7735, -122.5154),  # Ocean Beach, San Francisco\n",
    "    (37.8299, -122.4194),  # Baker Beach, San Francisco\n",
    "    (38.0573, -122.7562),  # Dillon Beach, Marin County\n",
    "\n",
    "    # Northern California\n",
    "    (39.3086, -123.8102),  # Fort Bragg, Mendocino County\n",
    "    (40.4358, -124.3945),  # Cape Mendocino\n",
    "    (41.7450, -124.1835)   # Crescent City Beach (near Oregon border)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "x_train_fe, y_train, preprocessing_pipeline = preprocess_data(train_data, coastline_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align columns in x_train and x_test by adding missing columns with NaN values\n",
    "missing_in_train = set(x_test.columns) - set(x_train_fe.columns)\n",
    "missing_in_test = set(x_train_fe.columns) - set(x_test.columns)\n",
    "\n",
    "# Add missing columns to x_train and x_test with NaN values\n",
    "for col in missing_in_train:\n",
    "    x_train_fe[col] = np.nan\n",
    "\n",
    "for col in missing_in_test:\n",
    "    x_test[col] = np.nan\n",
    "\n",
    "# Reorder columns so that they match\n",
    "x_train = x_train_fe[x_test.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join x_train with y_train, compute correlations, and sort by the target correlation in descending order\n",
    "correlation_check = x_train.join(y_train).corr()['median_house_value'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted correlations\n",
    "print(correlation_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Full Pipeline**: Including both preprocessing and model\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model on the preprocessed data\n",
    "full_pipeline.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['total_rooms'] = np.log(x_test['total_rooms'] + 1) \n",
    "x_test['total_bedrooms'] = np.log(x_test['total_bedrooms'] + 1)\n",
    "x_test['population'] = np.log(x_test['population'] + 1)\n",
    "x_test['households'] = np.log(x_test['households'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = full_pipeline.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training \n",
    "\n",
    "We also need to splitt data, we do not ened to do the train/test split again but me must do the x/y split again\n",
    "Also we must  add the new features to the test_data to evaluate the model \n",
    "\n",
    "not doing hyperparameter training for linear regression, saying it is okay for it to go into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the same except we scale our data\n",
    "we only need to scale the input, not the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a different model to linear regression, Random Forest Regressor\n",
    "ensemble is a different type of mL where we combine multiple types of models\n",
    "    in this case decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model_training\n",
    "importlib.reload(model_training)\n",
    "from model_training import get_model_pipelines, evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = get_model_pipelines(preprocessing_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
    "\n",
    " \n",
    "    pipeline.fit(x_train, y_train)  # Train the model\n",
    "    y_pred = pipeline.predict(x_test) \n",
    "\n",
    "    print(f\"Evaluation for {model_name}:\")\n",
    "    metrics = evaluate_model(y_test, y_pred)  # Evaluate predictions\n",
    "    \n",
    "    # Store results for comparison\n",
    "    results[model_name] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few actual values (y_test):\", y_test[:5])\n",
    "print(\"First few predicted values (y_pred):\", y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the updated evaluation metrics, the LightGBM model demonstrates the best performance among the tested models:\n",
    "\n",
    "***LightGBM:***\n",
    "\n",
    "Mean Squared Error (MSE): 11,574,950,000.00\n",
    "\n",
    "Root Mean Squared Error (RMSE): 107,587.00\n",
    "\n",
    "Mean Absolute Error (MAE): 80,157.93\n",
    "\n",
    "R² Score: 0.143\n",
    "\n",
    "This model has the lowest MSE, RMSE, and MAE among the models evaluated, as well as the highest R^2 score (0.143). R^2 value indicates that the model explains a modest proportion of the variance in the target, it outperforms alternatives like Gradient Boosting and XGBoost. These results suggest that LightGBM captures the target variable's underlying patterns more effectively than other models, though further improvements may be needed to enhance predictive performance across all models.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM Regressor pipeline\n",
    "pipeline_lgb = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # your existing preprocessing pipeline\n",
    "    ('model', lgb.LGBMRegressor(random_state=42))  # model to tune\n",
    "])\n",
    "\n",
    "# Define the refined parameter grid for LightGBM\n",
    "param_dist_lgb2 = {\n",
    "    'model__n_estimators': [700, 800, 900, 1000],  # increased number of boosting stages\n",
    "    'model__learning_rate': [0.06, 0.08, 0.1],  # fine-tuned learning rates\n",
    "    'model__num_leaves': [60, 70, 80],  # close to best range found before\n",
    "    'model__max_depth': [12, 14, 16],  # fine-tuning around the best depth\n",
    "    'model__min_data_in_leaf': [10, 15, 20],  # keeping min_data_in_leaf for balance\n",
    "    'model__subsample': [0.85, 0.9, 0.95, 1.0],  # adjust subsampling\n",
    "    'model__colsample_bytree': [0.5, 0.55, 0.6, 0.7],  # slightly lower feature sampling\n",
    "    'model__reg_alpha': [0.05, 0.1, 0.2],  # regularization parameter for potential minor improvements\n",
    "    'model__reg_lambda': [0.05, 0.1, 0.2]  # regularization parameter fine-tuning\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for LightGBM with adjusted search space\n",
    "random_search_lgb2 = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,  # the pipeline\n",
    "    param_distributions=param_dist_lgb2,  # refined hyperparameter grid\n",
    "    n_iter=30,  # reducing iterations to manage time, focusing on refined params\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # minimize mean squared error\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_lgb2.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for Refined LightGBM:\", random_search_lgb2.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb2.best_score_)\n",
    "\n",
    "# Evaluate the model using the best parameters\n",
    "best_lgb_model2 = random_search_lgb2.best_estimator_\n",
    "\n",
    "# Predictions with the best model\n",
    "lgb_predictions2 = best_lgb_model2.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, lgb_predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameter Tuning for LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can try increase the accuracy of the mondel by giving it a parameter grid and trying the different options \n",
    "\n",
    "lets use grid search cross validation\n",
    "\n",
    "splitting it into k-folds, use all but 1 fold to train thedata and 1 for the validation \n",
    "and then evaluate the parameters we used\n",
    "\n",
    "when we provide a parameter grid it is going to do the cross validation with all the different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# LightGBM pipeline with further narrowed parameter grid\n",
    "pipeline_lgb_final = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # Existing preprocessing pipeline\n",
    "    ('model', lgb.LGBMRegressor(random_state=42))  # Model to tune\n",
    "])\n",
    "\n",
    "# Focused parameter grid for LightGBM\n",
    "param_dist_lgb_final = {\n",
    "    'model__n_estimators': [600, 700, 800],  # Slightly increase n_estimators\n",
    "    'model__learning_rate': [0.08, 0.1, 0.12],  # Fine-tune learning rate\n",
    "    'model__num_leaves': [55, 60, 65],  # Adjust num_leaves around best value\n",
    "    'model__max_depth': [10, 12, 14],  # Small tweaks to max depth\n",
    "    'model__min_data_in_leaf': [10, 15, 20],  # Narrow down min_data_in_leaf\n",
    "    'model__subsample': [0.85, 0.9, 0.95],  # Slight increase for subsampling\n",
    "    'model__colsample_bytree': [0.55, 0.6, 0.65],  # Fine-tune colsample_bytree\n",
    "    'model__reg_alpha': [0.05, 0.1, 0.15],  # Further narrow L1 regularization\n",
    "    'model__reg_lambda': [0.05, 0.1, 0.15],  # Further narrow L2 regularization\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for LightGBM\n",
    "random_search_lgb_final = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb_final,\n",
    "    param_distributions=param_dist_lgb_final,\n",
    "    n_iter=20,  # Smaller iteration count for final targeted tuning\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_lgb_final.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for Refined LightGBM:\", random_search_lgb_final.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb_final.best_score_)\n",
    "\n",
    "# Evaluate the model with the refined best parameters\n",
    "best_lgb_model_final = random_search_lgb_final.best_estimator_\n",
    "\n",
    "# Predictions with the final model\n",
    "lgb_predictions_final = best_lgb_model_final.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, lgb_predictions_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined Parameter Grid for Minor Adjustments\n",
    "param_dist_lgb_refined = {\n",
    "    'model__n_estimators': [600, 650, 700],\n",
    "    'model__learning_rate': [0.07, 0.08],\n",
    "    'model__num_leaves': [60, 65],\n",
    "    'model__max_depth': [13, 14],\n",
    "    'model__min_data_in_leaf': [10, 15],\n",
    "    'model__subsample': [0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.5, 0.55],\n",
    "    'model__reg_alpha': [0.05, 0.1],\n",
    "    'model__reg_lambda': [0.1, 0.15]\n",
    "}\n",
    "\n",
    "# Setup RandomizedSearchCV for LightGBM with Refined Parameters\n",
    "random_search_lgb_refined = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,  \n",
    "    param_distributions=param_dist_lgb_refined, \n",
    "    n_iter=15,  # Limiting to 15 iterations for efficiency\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  \n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV with refined parameters\n",
    "random_search_lgb_refined.fit(x_train, y_train)\n",
    "\n",
    "# Output best parameters and model evaluation\n",
    "print(\"Best Parameters for Refined LightGBM:\", random_search_lgb_refined.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb_refined.best_score_)\n",
    "\n",
    "# Evaluate with best model\n",
    "best_lgb_model_refined = random_search_lgb_refined.best_estimator_\n",
    "lgb_predictions_refined = best_lgb_model_refined.predict(x_test)\n",
    "evaluate_model(y_test, lgb_predictions_refined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Create the LightGBM Regressor pipeline\n",
    "pipeline_lgb = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('model', lgb.LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Refined hyperparameter grid\n",
    "param_dist_lgb_optimized = {\n",
    "    'model__n_estimators': [600, 650, 700, 750],\n",
    "    'model__learning_rate': [0.06, 0.07, 0.08],\n",
    "    'model__num_leaves': [55, 60, 65],\n",
    "    'model__max_depth': [12, 13, 14],\n",
    "    'model__min_data_in_leaf': [8, 10, 15],\n",
    "    'model__subsample': [0.85, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.5, 0.55, 0.6],\n",
    "    'model__reg_alpha': [0.03, 0.05, 0.1],\n",
    "    'model__reg_lambda': [0.08, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "# Early stopping with a validation set\n",
    "random_search_lgb_optimized = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,\n",
    "    param_distributions=param_dist_lgb_optimized,\n",
    "    n_iter=12,  # Fewer iterations for speed\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the model with early stopping\n",
    "random_search_lgb_optimized.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate and display results\n",
    "print(\"Best Parameters for Optimized LightGBM:\", random_search_lgb_optimized.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb_optimized.best_score_)\n",
    "\n",
    "# Use the best model for predictions and evaluation\n",
    "best_lgb_model_optimized = random_search_lgb_optimized.best_estimator_\n",
    "lgb_predictions_optimized = best_lgb_model_optimized.predict(x_test)\n",
    "evaluate_model(y_test, lgb_predictions_optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Create the LightGBM Regressor pipeline\n",
    "pipeline_lgb = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('model', lgb.LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Further optimized hyperparameter grid\n",
    "param_dist_lgb_optimized = {\n",
    "    'model__n_estimators': [600, 650, 700],  # Narrower range\n",
    "    'model__learning_rate': [0.065, 0.07, 0.075],  # Fine-tuned range\n",
    "    'model__num_leaves': [60, 62, 65],  # Fewer options near the best\n",
    "    'model__max_depth': [13, 14],  # Reduced range to balance complexity\n",
    "    'model__min_data_in_leaf': [8, 10],  # Simplified for faster training\n",
    "    'model__subsample': [0.85, 0.9],  # Optimized based on diminishing returns\n",
    "    'model__colsample_bytree': [0.5, 0.55],  # Further restricted\n",
    "    'model__reg_alpha': [0.04, 0.05],  # Fine-grained regularization\n",
    "    'model__reg_lambda': [0.08, 0.1]  # Focused range\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV with fewer iterations and folds for efficiency\n",
    "random_search_lgb_optimized = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,\n",
    "    param_distributions=param_dist_lgb_optimized,\n",
    "    n_iter=8,  # Reduced for speed\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Utilize all available cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',  # Scoring metric\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the model with RandomizedSearchCV\n",
    "random_search_lgb_optimized.fit(x_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters for Optimized LightGBM:\", random_search_lgb_optimized.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb_optimized.best_score_)\n",
    "\n",
    "# Use the best model for predictions and evaluation\n",
    "best_lgb_model_optimized = random_search_lgb_optimized.best_estimator_\n",
    "lgb_predictions_optimized = best_lgb_model_optimized.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(y_test, lgb_predictions_optimized)\n",
    "print(\"Optimized Model Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_lgb_improved = {\n",
    "    'model__n_estimators': [650, 700, 750],  # Slightly increased to explore a broader range\n",
    "    'model__learning_rate': [0.06, 0.07, 0.075],  # Fine-tuned around previous best values\n",
    "    'model__num_leaves': [58, 60, 62],  # Narrowed for better exploration around the optimal range\n",
    "    'model__max_depth': [12, 13, 14],  # Maintaining similar depth but slightly narrowed\n",
    "    'model__min_data_in_leaf': [8, 10, 12],  # Focus on smaller leaf sizes for more granularity\n",
    "    'model__subsample': [0.9, 0.95, 1.0],  # Slightly more exploration at high subsampling levels\n",
    "    'model__colsample_bytree': [0.5, 0.55, 0.6],  # Consistent with earlier ranges\n",
    "    'model__reg_alpha': [0.03, 0.05, 0.08],  # Further tuned to minimize over-regularization\n",
    "    'model__reg_lambda': [0.08, 0.1, 0.12],  # Adjusted for optimal balance between L2 penalties\n",
    "}\n",
    "\n",
    "random_search_lgb_improved = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,  \n",
    "    param_distributions=param_dist_lgb_improved, \n",
    "    n_iter=15,  # Limited iterations for efficiency\n",
    "    cv=5,  # 5-fold cross-validation for robustness\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  \n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search_lgb_improved.fit(x_train, y_train)\n",
    "\n",
    "# Output the best parameters and performance metrics\n",
    "print(\"Best Parameters for Improved LightGBM:\", random_search_lgb_improved.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_lgb_improved.best_score_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_lgb_model_improved = random_search_lgb_improved.best_estimator_\n",
    "lgb_predictions_improved = best_lgb_model_improved.predict(x_test)\n",
    "results = evaluate_model(y_test, lgb_predictions_improved)\n",
    "print(\"Improved Model Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_lgb_final = {\n",
    "    'model__n_estimators': [600, 650],  # Focused on previously optimal range\n",
    "    'model__learning_rate': [0.07, 0.08],  # Narrow for stability\n",
    "    'model__num_leaves': [55, 60],  # Reduce upper limit\n",
    "    'model__max_depth': [13, 14],  # Stick to optimal depths\n",
    "    'model__min_data_in_leaf': [12, 15],  # More conservative lower bound\n",
    "    'model__subsample': [0.9, 0.95],  # Slightly less aggressive\n",
    "    'model__colsample_bytree': [0.5, 0.55],  # Balanced feature usage\n",
    "    'model__reg_alpha': [0.05, 0.1],  # Return to stable range\n",
    "    'model__reg_lambda': [0.1, 0.15]  # Prior optimal range\n",
    "}\n",
    "\n",
    "# Setup RandomizedSearchCV for LightGBM with the refined parameter grid\n",
    "random_search_lgb_final = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,\n",
    "    param_distributions=param_dist_lgb_final,\n",
    "    n_iter=10,  # Reduced iterations for efficiency\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV with refined parameters\n",
    "random_search_lgb_final.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate with the best model\n",
    "best_lgb_model_final = random_search_lgb_final.best_estimator_\n",
    "lgb_predictions_final = best_lgb_model_final.predict(x_test)\n",
    "evaluate_model(y_test, lgb_predictions_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline with preprocessing and the model (without early stopping)\n",
    "pipeline_lgb = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('model', lgb.LGBMRegressor(random_state=42, n_estimators=1000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_dist_lgb_refined = {\n",
    "    'model__n_estimators': [650, 700, 750, 800],  # Number of boosting rounds\n",
    "    'model__learning_rate': [0.055, 0.06, 0.065, 0.07],  # Learning rate\n",
    "    'model__num_leaves': [56, 58, 60, 62],  # Number of leaves in a tree\n",
    "    'model__max_depth': [12, 13, 14],  # Max depth of trees\n",
    "    'model__min_data_in_leaf': [8, 10, 12, 15],  # Minimum samples per leaf\n",
    "    'model__subsample': [0.85, 0.9, 0.95],  # Fraction of samples for training\n",
    "    'model__colsample_bytree': [0.5, 0.55, 0.6],  # Fraction of features per tree\n",
    "    'model__reg_alpha': [0.03, 0.04, 0.05, 0.08],  # L1 regularization\n",
    "    'model__reg_lambda': [0.08, 0.1, 0.12, 0.15],  # L2 regularization\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for hyperparameter tuning\n",
    "random_search_lgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgb,\n",
    "    param_distributions=param_dist_lgb_refined,\n",
    "    n_iter=20,  # Number of parameter settings to try\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    verbose=1,  # Print progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE as the metric for evaluation\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV with the hyperparameter grid\n",
    "random_search_lgb.fit(x_train, y_train)\n",
    "\n",
    "# Get the best model and make predictions\n",
    "best_lgb_model = random_search_lgb.best_estimator_\n",
    "lgb_predictions = best_lgb_model.predict(x_test)\n",
    "\n",
    "evaluate_model(y_test, lgb_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameter Tuning for XGBoost***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the XGBoost Regressor pipeline\n",
    "pipeline_xgb1 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # your existing preprocessing pipeline\n",
    "    ('model', XGBRegressor(random_state=42))  # model to tune\n",
    "])\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_dist_xgb1 = {\n",
    "    'model__n_estimators': [100, 200, 500],  # number of boosting rounds\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],  # shrinkage rate\n",
    "    'model__max_depth': [4, 6, 8, 10],  # depth of trees\n",
    "    'model__reg_lambda': [0.1, 1, 5],  # L2 regularization\n",
    "    'model__reg_alpha': [0, 0.1, 1],  # L1 regularization\n",
    "    'model__subsample': [0.6, 0.8, 1.0],  # fraction of samples used for each tree\n",
    "    'model__colsample_bytree': [0.6, 0.8, 1.0],  # fraction of features used per tree\n",
    "    'model__gamma': [0, 0.1, 0.3],  # minimum loss reduction required to make a split\n",
    "    'model__min_child_weight': [1, 3, 5],  # minimum sum of instance weight in a child\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for XGBoost\n",
    "random_search_xgb1 = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb1,  # the pipeline\n",
    "    param_distributions=param_dist_xgb1,  # hyperparameter grid\n",
    "    n_iter=20,  # number of parameter settings to sample\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # we want to minimize mean squared error\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_xgb1.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for XGBoost:\", random_search_xgb1.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_xgb1.best_score_)\n",
    "\n",
    "# Now evaluate the model using the best parameters\n",
    "best_xgb_model1 = random_search_xgb1.best_estimator_\n",
    "\n",
    "# Predictions with the best model\n",
    "xgb_predictions1 = best_xgb_model1.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, xgb_predictions1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the XGBoost Regressor pipeline\n",
    "pipeline_xgb_final = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # your existing preprocessing pipeline\n",
    "    ('model', xgb.XGBRegressor(random_state=42, verbosity=0))  # model to tune\n",
    "])\n",
    "\n",
    "# Refined parameter grid for XGBoost\n",
    "param_dist_xgb_final = {\n",
    "    'model__n_estimators': [300, 400, 500, 600],  # Lower max to keep runtime reasonable\n",
    "    'model__learning_rate': [0.03, 0.05, 0.07],  # Smaller steps for learning rate\n",
    "    'model__max_depth': [6, 7, 8, 9],  # Narrower range to balance model complexity\n",
    "    'model__min_child_weight': [3, 5, 7],  # Moderate values to avoid overfitting\n",
    "    'model__subsample': [0.7, 0.8, 0.9, 1.0],  # Allow some flexibility in sampling\n",
    "    'model__colsample_bytree': [0.5, 0.6, 0.7],  # Fraction of columns to use for training\n",
    "    'model__gamma': [0, 0.1, 0.3, 0.5],  # Regularization term to reduce overfitting\n",
    "    'model__reg_alpha': [0.05, 0.1, 0.2],  # L1 regularization\n",
    "    'model__reg_lambda': [0.05, 0.1, 0.2],  # L2 regularization\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for XGBoost with refined parameters\n",
    "random_search_xgb_final = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb_final,  # the pipeline\n",
    "    param_distributions=param_dist_xgb_final,  # hyperparameter grid\n",
    "    n_iter=20,  # Limit to 20 iterations for efficiency\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # minimize MSE\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_xgb_final.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for XGBoost:\", random_search_xgb_final.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_xgb_final.best_score_)\n",
    "\n",
    "# Now evaluate the model using the best parameters\n",
    "best_xgb_model_final = random_search_xgb_final.best_estimator_\n",
    "\n",
    "# Predictions with the best model\n",
    "xgb_predictions_final = best_xgb_model_final.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, xgb_predictions_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the XGBoost Regressor pipeline\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # your existing preprocessing pipeline\n",
    "    ('model', XGBRegressor(random_state=42))  # XGBoost model to tune\n",
    "])\n",
    "\n",
    "# Define a refined parameter grid for XGBoost\n",
    "param_dist_xgb2 = {\n",
    "    'model__n_estimators': [500, 550, 600],  # Slightly higher values to ensure sufficient boosting rounds\n",
    "    'model__learning_rate': [0.02, 0.03, 0.04],  # Smaller increments around best\n",
    "    'model__max_depth': [7, 8, 9],  # Depth slightly tuned around current best\n",
    "    'model__min_child_weight': [6, 7, 8],  # Focusing near current optimum\n",
    "    'model__subsample': [0.85, 0.9, 0.95],  # Subsampling around previous best values\n",
    "    'model__colsample_bytree': [0.5, 0.55, 0.6],  # Slight adjustments in colsample_bytree\n",
    "    'model__reg_alpha': [0.04, 0.05, 0.06],  # L1 regularization refined around best values\n",
    "    'model__reg_lambda': [0.04, 0.05, 0.06],  # L2 regularization also refined\n",
    "    'model__gamma': [0, 0.1, 0.2]  # Testing a few small values for gamma\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for refined XGBoost tuning\n",
    "random_search_xgb2 = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,  # the pipeline\n",
    "    param_distributions=param_dist_xgb2,  # hyperparameter grid\n",
    "    n_iter=15,  # Limiting the iterations for efficiency\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # Minimizing mean squared error\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_xgb2.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for Refined XGBoost:\", random_search_xgb2.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_xgb2.best_score_)\n",
    "\n",
    "# Now evaluate the model using the best parameters\n",
    "best_xgb_model2 = random_search_xgb2.best_estimator_\n",
    "\n",
    "# Predictions with the best model\n",
    "xgb_predictions2 = best_xgb_model2.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, xgb_predictions2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the Random Forest Regressor pipeline\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),  # your existing preprocessing pipeline\n",
    "    ('model', RandomForestRegressor(random_state=42))  # model to tune\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_dist_rf = {\n",
    "    'model__n_estimators': [100, 200, 300, 400, 500],  # number of trees\n",
    "    'model__max_depth': [None, 10, 20, 30, 40],  # maximum depth of trees\n",
    "    'model__min_samples_split': [2, 5, 10],  # min samples needed to split a node\n",
    "    'model__min_samples_leaf': [1, 2, 4],  # min samples needed to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2'],  # max number of features used per tree\n",
    "    'model__bootstrap': [True, False]  # whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for Random Forest\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf,  # the pipeline\n",
    "    param_distributions=param_dist_rf,  # hyperparameter grid\n",
    "    n_iter=20,  # number of parameter settings to sample\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # we want to minimize mean squared error\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_rf.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score from RandomizedSearchCV\n",
    "print(\"Best Parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", random_search_rf.best_score_)\n",
    "\n",
    "# Now evaluate the model using the best parameters\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "\n",
    "# Predictions with the best model\n",
    "rf_predictions = best_rf_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluate_model(y_test, rf_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the best weights and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb_model_final = best_lgb_model_improved\n",
    "best_xgb_model_final = best_xgb_model2\n",
    "\n",
    "# Get predictions from the best models\n",
    "lgb_test_preds = best_lgb_model_final.predict(x_test)\n",
    "xgb_test_preds = best_xgb_model_final.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the range of weights to test\n",
    "weight_range = np.linspace(0.1, 0.9, 9)  # 0.1 to 0.9 in steps of 0.1\n",
    "\n",
    "# Store the results for each weight combination\n",
    "best_weight = None\n",
    "best_r2 = -np.inf  # Start with the lowest possible R²\n",
    "best_metrics = None\n",
    "\n",
    "print(\"Tuning weights for Weighted Averaging Ensemble...\")\n",
    "for weight in weight_range:\n",
    "    # Define weights for LightGBM and XGBoost\n",
    "    weights = [weight, 1 - weight]\n",
    "\n",
    "    # Compute weighted predictions\n",
    "    weighted_predictions = (\n",
    "        weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    "    )\n",
    "\n",
    "    # Evaluate performance\n",
    "    mse = mean_squared_error(y_test, weighted_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, weighted_predictions)\n",
    "    r2 = r2_score(y_test, weighted_predictions)\n",
    "\n",
    "    # Update the best weights if R² improves\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_weight = weights\n",
    "        best_metrics = (mse, rmse, mae, r2)\n",
    "\n",
    "# Print the best weights and corresponding performance metrics\n",
    "print(\"\\nBest Weights for Weighted Averaging:\")\n",
    "print(f\"LightGBM Weight: {best_weight[0]}\")\n",
    "print(f\"XGBoost Weight: {best_weight[1]}\")\n",
    "print(\"\\nBest Weighted Averaging Ensemble Performance:\")\n",
    "print(f\"Mean Squared Error: {best_metrics[0]}\")\n",
    "print(f\"Root Mean Squared Error: {best_metrics[1]}\")\n",
    "print(f\"Mean Absolute Error: {best_metrics[2]}\")\n",
    "print(f\"R² Score: {best_metrics[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE and RMSE are relatively high, indicating the model's predictions are still somewhat far from the actual values, but the error is not unreasonably large.\n",
    "\n",
    "MAE suggests that the average prediction error in the units of the target variable is around 65,600, which is a useful indicator of the model's predictive accuracy.\n",
    "\n",
    "R² Score of 0.434 means that about 43.4% of the variance in the target variable is explained by the weighted averaging ensemble, which is a good result, but there's still room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define the path to the models folder (relative to the current working directory)\n",
    "models_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# Ensure the models directory exists\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Finalize the optimal weights\n",
    "optimal_weights = [0.9, 0.1]\n",
    "\n",
    "# Final weighted predictions\n",
    "final_weighted_predictions = (\n",
    "    optimal_weights[0] * lgb_test_preds + optimal_weights[1] * xgb_test_preds\n",
    ")\n",
    "\n",
    "# Evaluate and save the performance\n",
    "print(\"Final Weighted Averaging Ensemble Performance:\")\n",
    "evaluate_model(y_test, final_weighted_predictions)\n",
    "\n",
    "# Save the models and weights for reproducibility\n",
    "with open(os.path.join(models_dir, 'best_lgb_model_final.pkl'), 'wb') as f:\n",
    "    pickle.dump(best_lgb_model_final, f)\n",
    "\n",
    "with open(os.path.join(models_dir, 'best_xgb_model_final.pkl'), 'wb') as f:\n",
    "    pickle.dump(best_xgb_model_final, f)\n",
    "\n",
    "with open(os.path.join(models_dir, 'optimal_weights.pkl'), 'wb') as f:\n",
    "    pickle.dump(optimal_weights, f)\n",
    "\n",
    "print(\"Finalized models and weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define the path to the models folder\n",
    "models_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# Load the saved models and weights\n",
    "with open(os.path.join(models_dir, 'best_lgb_model_final.pkl'), 'rb') as f:\n",
    "    best_lgb_model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(models_dir, 'best_xgb_model_final.pkl'), 'rb') as f:\n",
    "    best_xgb_model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(models_dir, 'optimal_weights.pkl'), 'rb') as f:\n",
    "    optimal_weights = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Averaging Ensemble (Simple)\n",
    "In an averaging ensemble, we simply average the predictions of both models and evaluate the performance. This is the simplest approach and can often work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the best LightGBM model on the entire training set\n",
    "best_lgb_model_final.fit(x_train, y_train)\n",
    "\n",
    "# Fit the best XGBoost model on the entire training set\n",
    "best_xgb_model_final.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions with each model\n",
    "ensemble_lgb_predictions = best_lgb_model_final.predict(x_test)\n",
    "ensemble_xgb_predictions = best_xgb_model_final.predict(x_test)\n",
    "\n",
    "# Create ensemble predictions by averaging the predictions\n",
    "ensemble_predictions = (ensemble_lgb_predictions + ensemble_xgb_predictions) / 2\n",
    "\n",
    "# Evaluate the ensemble\n",
    "mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, ensemble_predictions)\n",
    "r2 = r2_score(y_test, ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign weights based on model performance (adjust based on results)\n",
    "weight_lgb = 0.01  # Assign higher weight if LightGBM performs better\n",
    "weight_xgb = 0.99\n",
    "\n",
    "# Create weighted predictions\n",
    "weighted_ensemble_predictions = (\n",
    "    weight_lgb * ensemble_lgb_predictions +\n",
    "    weight_xgb * ensemble_xgb_predictions\n",
    ")\n",
    "\n",
    "# Evaluate weighted ensemble\n",
    "mse = mean_squared_error(y_test, weighted_ensemble_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, weighted_ensemble_predictions)\n",
    "r2 = r2_score(y_test, weighted_ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Weighted Ensemble Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stacking Ensemble (Advanced)\n",
    "In stacking, you train a \"meta-model\" on the predictions of the base models (i.e., XGBoost and LightGBM in this case). The base models' predictions become input features for the meta-model. Typically, you use a simple linear model (like Ridge or Logistic Regression) as the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Get predictions from both base models (LightGBM and XGBoost)\n",
    "lgb_train_preds = best_lgb_model_final.predict(x_train)\n",
    "xgb_train_preds = best_xgb_model_final.predict(x_train)\n",
    "\n",
    "# Stack predictions together to form a new dataset\n",
    "stacked_train_preds = np.vstack((lgb_train_preds, xgb_train_preds)).T\n",
    "\n",
    "# Use a simple Ridge Regression as the meta-model\n",
    "meta_model = Ridge()\n",
    "\n",
    "# Train the meta-model on the predictions of the base models\n",
    "meta_model.fit(stacked_train_preds, y_train)\n",
    "\n",
    "# Now make predictions using both models and the meta-model\n",
    "lgb_test_preds = best_lgb_model_final.predict(x_test)\n",
    "xgb_test_preds = best_xgb_model_final.predict(x_test)\n",
    "\n",
    "# Stack the predictions of both models for the test set\n",
    "stacked_test_preds = np.vstack((lgb_test_preds, xgb_test_preds)).T\n",
    "\n",
    "# Make the final predictions using the meta-model\n",
    "final_predictions = meta_model.predict(stacked_test_preds)\n",
    "\n",
    "# Evaluate the performance of the stacked model\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Stacked Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **1. Stacking Ensemble with Ridge Meta-Model**\n",
    "# Get predictions from both base models (LightGBM and XGBoost)\n",
    "lgb_train_preds = best_lgb_model_final.predict(x_train)\n",
    "xgb_train_preds = best_xgb_model_final.predict(x_train)\n",
    "\n",
    "# Stack predictions together to form a new dataset\n",
    "stacked_train_preds = np.vstack((lgb_train_preds, xgb_train_preds)).T\n",
    "\n",
    "# Hyperparameter tuning for Ridge Regression as the meta-model\n",
    "param_grid_meta = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100]  # Ridge regularization strength\n",
    "}\n",
    "grid_meta = GridSearchCV(\n",
    "    Ridge(),\n",
    "    param_grid_meta,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_meta.fit(stacked_train_preds, y_train)\n",
    "\n",
    "# Best Ridge meta-model\n",
    "best_meta_model = grid_meta.best_estimator_\n",
    "\n",
    "# Train the tuned meta-model on the stacked predictions\n",
    "best_meta_model.fit(stacked_train_preds, y_train)\n",
    "\n",
    "# Now make predictions using both models and the tuned meta-model\n",
    "lgb_test_preds = best_lgb_model_final.predict(x_test)\n",
    "xgb_test_preds = best_xgb_model_final.predict(x_test)\n",
    "\n",
    "# Stack the predictions of both models for the test set\n",
    "stacked_test_preds = np.vstack((lgb_test_preds, xgb_test_preds)).T\n",
    "\n",
    "# Make the final predictions using the meta-model\n",
    "final_predictions = best_meta_model.predict(stacked_test_preds)\n",
    "\n",
    "# Evaluate the performance of the stacked model\n",
    "print(\"Tuned Stacking Model Performance:\")\n",
    "evaluate_model(y_test, final_predictions)\n",
    "\n",
    "# **2. Weighted Averaging Ensemble**\n",
    "# Adjust weights to prioritize LightGBM over XGBoost\n",
    "weights = [0.6, 0.4]\n",
    "\n",
    "# Compute weighted predictions\n",
    "averaged_predictions_weighted = (\n",
    "    weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    ")\n",
    "\n",
    "# Evaluate the weighted averaging ensemble\n",
    "print(\"Weighted Averaging Ensemble Performance:\")\n",
    "evaluate_model(y_test, averaged_predictions_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "# Define the range of weights to test\n",
    "weight_range = np.linspace(0.1, 0.9, 9)  # Test weights from 0.1 to 0.9\n",
    "\n",
    "best_weight_combination = None\n",
    "best_r2 = -np.inf\n",
    "best_metrics = None\n",
    "\n",
    "# Iterate through weight combinations for LightGBM and XGBoost\n",
    "for weight in weight_range:\n",
    "    # Define weights for LightGBM and XGBoost\n",
    "    weights = [weight, 1 - weight]\n",
    "    \n",
    "    # Weighted train predictions\n",
    "    weighted_train_preds = (\n",
    "        weights[0] * lgb_train_preds + weights[1] * xgb_train_preds\n",
    "    )\n",
    "    \n",
    "    # Fit the Ridge meta-model on weighted train predictions\n",
    "    param_grid_meta = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "    grid_meta = GridSearchCV(\n",
    "        Ridge(),\n",
    "        param_grid_meta,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_meta.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    best_meta_model = grid_meta.best_estimator_\n",
    "    best_meta_model.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # Weighted test predictions\n",
    "    weighted_test_preds = (\n",
    "        weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    "    )\n",
    "    \n",
    "    # Stack test predictions for final predictions\n",
    "    stacked_test_preds = weighted_test_preds.reshape(-1, 1)\n",
    "    final_predictions = best_meta_model.predict(stacked_test_preds)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    metrics = evaluate_model(y_test, final_predictions)\n",
    "    r2 = metrics['R2']\n",
    "    \n",
    "    # Track the best weight combination\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_weight_combination = weights\n",
    "        best_metrics = metrics\n",
    "\n",
    "# Print the best weights and corresponding performance\n",
    "print(\"\\nBest Weights for Weighted Stacking Ensemble:\")\n",
    "print(f\"LightGBM Weight: {best_weight_combination[0]}\")\n",
    "print(f\"XGBoost Weight: {best_weight_combination[1]}\")\n",
    "print(\"\\nBest Weighted Stacking Ensemble Performance:\")\n",
    "print(best_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of weights to test for LightGBM and XGBoost\n",
    "weight_range = np.linspace(0.1, 0.9, 9)  # Test weights from 0.1 to 0.9 for LightGBM\n",
    "\n",
    "# Initialize variables to track the best results\n",
    "best_weight_combination = None\n",
    "best_r2 = -np.inf\n",
    "best_metrics = None\n",
    "\n",
    "# Loop through weight combinations for LightGBM and XGBoost\n",
    "for weight in weight_range:\n",
    "    # Define weights for LightGBM and XGBoost (LightGBM gets weight, XGBoost gets 1 - weight)\n",
    "    weights = [weight, 1 - weight]\n",
    "    \n",
    "    # Create weighted train predictions by applying weights\n",
    "    weighted_train_preds = (\n",
    "        weights[0] * lgb_train_preds + weights[1] * xgb_train_preds\n",
    "    )\n",
    "    \n",
    "    # Define a range of alpha values for Ridge regularization (try a wider range of values)\n",
    "    param_grid_meta = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 100]  # Wider range for more flexibility\n",
    "    }\n",
    "    \n",
    "    # Perform GridSearchCV to tune the Ridge model\n",
    "    grid_meta = GridSearchCV(\n",
    "        Ridge(),\n",
    "        param_grid_meta,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the Ridge meta-model on the weighted training predictions\n",
    "    grid_meta.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    best_meta_model = grid_meta.best_estimator_\n",
    "    best_meta_model.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # Create weighted test predictions using the same weights\n",
    "    weighted_test_preds = (\n",
    "        weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    "    )\n",
    "    \n",
    "    # Stack the test predictions for final output\n",
    "    stacked_test_preds = weighted_test_preds.reshape(-1, 1)\n",
    "    final_predictions = best_meta_model.predict(stacked_test_preds)\n",
    "    \n",
    "    # Evaluate the model's performance\n",
    "    metrics = evaluate_model(y_test, final_predictions)\n",
    "    r2 = metrics['R2']\n",
    "    \n",
    "    # Track the best weight combination and performance based on R² score\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_weight_combination = weights\n",
    "        best_metrics = metrics\n",
    "\n",
    "# Print the best weight combination and corresponding performance metrics\n",
    "print(\"\\nBest Weights for Weighted Stacking Ensemble:\")\n",
    "print(f\"LightGBM Weight: {best_weight_combination[0]}\")\n",
    "print(f\"XGBoost Weight: {best_weight_combination[1]}\")\n",
    "print(\"\\nBest Weighted Stacking Ensemble Performance:\")\n",
    "print(best_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Improvement: With an R² score of around 0.44, the model is capturing a fair amount of variance in the target variable. While it's not perfect, the ensemble method has clearly improved performance compared to individual models, and it is a good starting point for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Using Neural Networks as the Meta-Model:***\n",
    "I replace the Ridge Regression meta-model with a Neural Network, using MLPRegressor from scikit-learn\n",
    "\n",
    "***2. Boosting as the Meta-Model:***\n",
    "Instead of a Ridge regression, I use a boosting method like XGBoost or LightGBM as the meta-model in the stacking ensemble. These models can capture more complex relationships in the data and may improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of weights to test for LightGBM and XGBoost\n",
    "weight_range = np.linspace(0.1, 0.9, 9)  # Test weights from 0.1 to 0.9 for LightGBM\n",
    "\n",
    "# Initialize variables to track the best results\n",
    "best_weight_combination = None\n",
    "best_r2 = -np.inf\n",
    "best_metrics = None\n",
    "\n",
    "# Loop through weight combinations for LightGBM and XGBoost\n",
    "for weight in weight_range:\n",
    "    # Define weights for LightGBM and XGBoost (LightGBM gets weight, XGBoost gets 1 - weight)\n",
    "    weights = [weight, 1 - weight]\n",
    "    \n",
    "    # Create weighted train predictions by applying weights\n",
    "    weighted_train_preds = (\n",
    "        weights[0] * lgb_train_preds + weights[1] * xgb_train_preds\n",
    "    )\n",
    "    \n",
    "    # **Option 1: Use a Neural Network as the Meta-Model (MLPRegressor)**\n",
    "    # Initialize the neural network regressor (MLP with 1 hidden layer, you can tune it further)\n",
    "    nn_meta_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    \n",
    "    # Fit the neural network on the weighted training predictions\n",
    "    nn_meta_model.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # **Option 2: Use XGBoost as the Meta-Model**\n",
    "    # Initialize an XGBoost model as the meta-learner\n",
    "    xgb_meta_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the XGBoost model on the weighted training predictions\n",
    "    xgb_meta_model.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # Weighted test predictions using the same weights\n",
    "    weighted_test_preds = (\n",
    "        weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    "    )\n",
    "    \n",
    "    # Stack the test predictions for final output\n",
    "    stacked_test_preds = weighted_test_preds.reshape(-1, 1)\n",
    "    \n",
    "    # Get final predictions from the neural network meta-model\n",
    "    nn_final_predictions = nn_meta_model.predict(stacked_test_preds)\n",
    "    \n",
    "    # Get final predictions from the XGBoost meta-model\n",
    "    xgb_final_predictions = xgb_meta_model.predict(stacked_test_preds)\n",
    "    \n",
    "    # Combine the predictions from both meta-models (could test weighted average of these too)\n",
    "    final_predictions = (nn_final_predictions + xgb_final_predictions) / 2\n",
    "    \n",
    "    # Evaluate the model's performance\n",
    "    mse = mean_squared_error(y_test, final_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, final_predictions)\n",
    "    r2 = r2_score(y_test, final_predictions)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    # Track the best weight combination and performance based on R² score\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_weight_combination = weights\n",
    "        best_metrics = metrics\n",
    "\n",
    "# Print the best weight combination and corresponding performance metrics\n",
    "print(\"\\nBest Weights for Weighted Stacking Ensemble:\")\n",
    "print(f\"LightGBM Weight: {best_weight_combination[0]}\")\n",
    "print(f\"XGBoost Weight: {best_weight_combination[1]}\")\n",
    "print(\"\\nBest Weighted Stacking Ensemble Performance:\")\n",
    "print(best_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Define the range of weights to test for LightGBM and XGBoost\n",
    "weight_range = np.linspace(0.05, 0.95, 19)  # Test weights from 0.05 to 0.95 for finer granularity\n",
    "\n",
    "best_weight_combination = None\n",
    "best_r2 = -np.inf\n",
    "best_metrics = None\n",
    "\n",
    "# Loop through weight combinations for LightGBM and XGBoost\n",
    "for weight in weight_range:\n",
    "    # Define weights for LightGBM and XGBoost\n",
    "    weights = [weight, 1 - weight]\n",
    "    \n",
    "    # Create weighted train predictions\n",
    "    weighted_train_preds = (\n",
    "        weights[0] * lgb_train_preds + weights[1] * xgb_train_preds\n",
    "    )\n",
    "    \n",
    "    # Define a wider grid for Ridge regularization\n",
    "    param_grid_meta = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 500]  # Wider range of regularization strength\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV for Ridge meta-model\n",
    "    grid_meta = GridSearchCV(\n",
    "        Ridge(),\n",
    "        param_grid_meta,\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_meta.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    best_meta_model = grid_meta.best_estimator_\n",
    "    \n",
    "    # Train meta-model on the weighted train predictions\n",
    "    best_meta_model.fit(weighted_train_preds.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # Create weighted test predictions\n",
    "    weighted_test_preds = (\n",
    "        weights[0] * lgb_test_preds + weights[1] * xgb_test_preds\n",
    "    )\n",
    "    \n",
    "    stacked_test_preds = weighted_test_preds.reshape(-1, 1)\n",
    "    final_predictions = best_meta_model.predict(stacked_test_preds)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    metrics = evaluate_model(y_test, final_predictions)\n",
    "    r2 = metrics['R2']\n",
    "    \n",
    "    # Track best weight combination based on R2 score\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_weight_combination = weights\n",
    "        best_metrics = metrics\n",
    "\n",
    "# Print the best weight combination and performance metrics\n",
    "print(\"\\nBest Weights for Weighted Stacking Ensemble:\")\n",
    "print(f\"LightGBM Weight: {best_weight_combination[0]}\")\n",
    "print(f\"XGBoost Weight: {best_weight_combination[1]}\")\n",
    "print(\"\\nBest Weighted Stacking Ensemble Performance:\")\n",
    "print(best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform cross validation\n",
    "Cross-validate the ensemble model on unseen data to ensure robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "california_houses = load_data(\"California_Houses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names in the new dataset\n",
    "print(california_houses.columns)\n",
    "\n",
    "# Identify missing or extra columns compared to the original dataset\n",
    "required_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                    'total_bedrooms', 'population', 'households', 'median_income',\n",
    "                    'ocean_proximity']\n",
    "missing_columns = set(required_columns) - set(california_houses.columns)\n",
    "extra_columns = set(california_houses.columns) - set(required_columns)\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Match Columns and Handle Missing Features***\n",
    "We need to rename, transform, or drop certain columns.\n",
    "The new dataset uses slightly different names for similar concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'Longitude': 'longitude',\n",
    "    'Latitude': 'latitude',\n",
    "    'Median_Age': 'housing_median_age',\n",
    "    'Tot_Rooms': 'total_rooms',\n",
    "    'Tot_Bedrooms': 'total_bedrooms',\n",
    "    'Population': 'population',\n",
    "    'Households': 'households',\n",
    "    'Median_Income': 'median_income',\n",
    "    'Median_House_Value': 'median_house_value'\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "california_houses.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "\n",
    "# we drop extra columns is consistency with the model's training data. \n",
    "# The ensemble model has only seen the original dataset's features during training. \n",
    "# Including new features at prediction time would confuse the model, as it has no learned parameters or relationships for those features.\n",
    "\n",
    "\n",
    "extra_columns = ['Distance_to_coast', 'Distance_to_LA', 'Distance_to_SanDiego', \n",
    "                'Distance_to_SanJose', 'Distance_to_SanFrancisco']\n",
    "california_houses.drop(columns=extra_columns, inplace=True)\n",
    "\n",
    "# Assign a placeholder value\n",
    "california_houses['ocean_proximity'] = 'NEAR BAY'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_california_houses = california_houses.drop(['median_house_value'], axis=1)\n",
    "y_california_houses = california_houses['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing pipeline from saved model\n",
    "with open('best_lgb_model_final.pkl', 'rb') as f:\n",
    "    best_lgb_model = pickle.load(f)\n",
    "\n",
    "preprocessing_pipeline = best_lgb_model.named_steps['preprocessing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering as a standalone step to update `x_california_houses` with new features.\n",
    "x_california_houses_fe = feature_engineering().fit_transform(x_california_houses)\n",
    "\n",
    "# Redefine numerical and categorical features based on the transformed data.\n",
    "numerical_features = x_california_houses_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = x_california_houses_fe.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align columns in x_train and x_california_houses by adding missing columns with NaN values\n",
    "missing_in_train = set(x_train.columns) - set(x_california_houses_fe.columns)\n",
    "missing_in_test = set(x_california_houses_fe.columns) - set(x_train.columns)\n",
    "\n",
    "# Add missing columns to x_california_houses and x_train with NaN values\n",
    "for col in missing_in_train:\n",
    "    x_california_houses_fe[col] = np.nan\n",
    "\n",
    "for col in missing_in_test:\n",
    "    x_train[col] = np.nan\n",
    "\n",
    "# Reorder columns so that they match\n",
    "x_california_houses = x_california_houses_fe[x_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to new data\n",
    "processed_new_data = preprocessing_pipeline.transform(x_california_houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved models and weights\n",
    "with open('best_xgb_model_final.pkl', 'rb') as f:\n",
    "    best_xgb_model = pickle.load(f)\n",
    "\n",
    "with open('optimal_weights.pkl', 'rb') as f:\n",
    "    optimal_weights = pickle.load(f)\n",
    "\n",
    "# Generate predictions for the new data\n",
    "lgb_predictions = best_lgb_model.named_steps['model'].predict(processed_new_data)\n",
    "xgb_predictions = best_xgb_model.named_steps['model'].predict(processed_new_data)\n",
    "\n",
    "# Weighted predictions\n",
    "final_predictions = (\n",
    "    optimal_weights[0] * lgb_predictions + optimal_weights[1] * xgb_predictions\n",
    ")\n",
    "\n",
    "# Print predictions\n",
    "print(\"Ensemble model predictions for new data:\")\n",
    "print(final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Set LightGBM parameters to resolve warnings (if re-training or initializing)\n",
    "lightgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_data_in_leaf': 10  # Ensure consistency\n",
    "}\n",
    "\n",
    "# Split into features and target\n",
    "X_new = processed_new_data\n",
    "y_new = y_california_houses\n",
    "\n",
    "# Define a scoring function\n",
    "scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Define ensemble prediction function\n",
    "def ensemble_predict(X):\n",
    "    lgb_preds = best_lgb_model.named_steps['model'].predict(X)\n",
    "    xgb_preds = best_xgb_model.named_steps['model'].predict(X)\n",
    "    return optimal_weights[0] * lgb_preds + optimal_weights[1] * xgb_preds\n",
    "\n",
    "# Wrap ensemble logic in a scikit-learn-compatible estimator\n",
    "class EnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, predict_function):\n",
    "        self.predict_function = predict_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self  # Pre-trained models; no fitting required\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_function(X)\n",
    "\n",
    "# Create a scikit-learn compatible ensemble regressor\n",
    "ensemble_model = EnsembleRegressor(predict_function=ensemble_predict)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, processed_new_data, y_california_houses, scoring=scorer, cv=5)\n",
    "\n",
    "# Output results\n",
    "print(f\"Cross-validation RMSE: {cv_scores.mean()} ± {cv_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finalize the Weighted Averaging Ensemble\n",
    "We'll save the optimal weights and ensure they are used for final predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
